# lm_generation/evaluate_topics.py

import re
import argparse
from pathlib import Path
from typing import List

try:
    from sentence_transformers import SentenceTransformer
    from sklearn.metrics.pairwise import cosine_similarity
    HAS_EMBEDDING = True
except ImportError:
    HAS_EMBEDDING = False

def standardize_topic(topic: str) -> str:
    topic = topic.strip()
    if not topic:
        return ""
    topic = re.sub(r"^[^\w\u4e00-\u9fa5â€œâ€â€˜â€™]+", "", topic)
    topic = re.sub(r"[^\w\u4e00-\u9fa5â€œâ€â€˜â€™]+$", "", topic)
    if topic.startswith("ä¸äººç›¸å¤„ï¼Œ"):
        core = topic.replace("ä¸äººç›¸å¤„ï¼Œ", "").strip()
        if "/" in core:
            p1, p2 = core.split("/", 1)
            return f"ä¸äººç›¸å¤„ï¼Œåº”è¯¥{p1.strip()}è¿˜æ˜¯{p2.strip()}ï¼Ÿ"
    if not topic.endswith(("ï¼Ÿ", "?")):
        topic = topic.rstrip("ã€‚ï¼!") + "ï¼Ÿ"
    return topic

def normalize_for_dedup(s: str) -> str:
    return re.sub(r"[^\w\u4e00-\u9fa5]", "", s).lower()

def semantic_dedup(topics: List[str], threshold: float = 0.85) -> List[str]:
    if not HAS_EMBEDDING or len(topics) < 2:
        return topics
    print("ğŸ” è®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦ï¼ˆé¦–æ¬¡è¿è¡Œä¼šä¸‹è½½æ¨¡å‹ï¼‰...")
    model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
    emb = model.encode(topics, show_progress_bar=True)
    sim = cosine_similarity(emb)
    keep = [True] * len(topics)
    for i in range(len(topics)):
        if not keep[i]: continue
        for j in range(i + 1, len(topics)):
            if sim[i][j] > threshold:
                keep[j] = False
    return [topics[i] for i in range(len(topics)) if keep[i]]

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--file", "-f", default="debate_topics.txt", help="è¾©é¢˜æ–‡ä»¶è·¯å¾„")
    args = parser.parse_args()

    file_path = Path(__file__).parent / args.file
    if not file_path.exists():
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        return

    # è¯»å–åŸå§‹å†…å®¹
    with open(file_path, 'r', encoding='utf-8') as f:
        lines = f.readlines()

    raw_topics = []
    comments = []
    for line in lines:
        stripped = line.strip()
        if stripped.startswith('#'):
            comments.append(line)  # ä¿ç•™æ³¨é‡Š
        elif stripped:
            raw_topics.append(stripped)

    print(f"ğŸ“¥ è¯»å– {len(raw_topics)} æ¡è¾©é¢˜")

    # æ ‡å‡†åŒ–
    standardized = [standardize_topic(t) for t in raw_topics]
    standardized = [t for t in standardized if t]

    # ç²¾ç¡®å»é‡ï¼ˆä¿æŒé¡ºåºï¼‰
    seen = set()
    unique = []
    for t in standardized:
        key = normalize_for_dedup(t)
        if key not in seen:
            seen.add(key)
            unique.append(t)
    print(f"ğŸ” ç²¾ç¡®å»é‡å: {len(unique)} æ¡")

    # è¯­ä¹‰å»é‡
    cleaned = semantic_dedup(unique)
    print(f"ğŸ§  è¯­ä¹‰å»é‡å: {len(cleaned)} æ¡")

    # è´¨é‡æç¤º
    bad_len = [t for t in cleaned if len(t) < 6 or len(t) > 60]
    no_q = [t for t in cleaned if "ï¼Ÿ" not in t and "?" not in t]
    if bad_len or no_q:
        print("âš ï¸  è­¦å‘Š:")
        if bad_len: print(f"  - é•¿åº¦å¼‚å¸¸: {len(bad_len)} æ¡")
        if no_q: print(f"  - ç¼ºå°‘é—®å·: {len(no_q)} æ¡")

    # å†™å›åŸæ–‡ä»¶ï¼ˆä¿ç•™æ³¨é‡Š + æ–°å†…å®¹ï¼‰
    with open(file_path, 'w', encoding='utf-8') as f:
        # å…ˆå†™æ³¨é‡Šï¼ˆå¦‚æœæœ‰ï¼‰
        for comment in comments:
            f.write(comment)
        if comments and cleaned:
            f.write('\n')
        # å†å†™æ¸…æ´—åçš„è¾©é¢˜
        for topic in cleaned:
            f.write(topic + '\n')

    print(f"\nâœ… æˆåŠŸæ¸…æ´—å¹¶æ›´æ–°: {file_path}")

if __name__ == "__main__":
    main()